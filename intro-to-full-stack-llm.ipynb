{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1UpdIFjr1o_"
   },
   "source": [
    "# Intro To Full Stack LLM Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "i2lzOJhQr1pC",
    "tags": []
   },
   "source": [
    "## Workshop Goals\n",
    "Our goal is to give you an understanding of:\n",
    "- The basic components that go into building your own ChatGPT-like app\n",
    "- The basic workflow of prototyping an AI app in JupyterLab\n",
    "- How to convert a JupyterLab prototype into an app that can run on a web server\n",
    "- How to run your web server app on your computer\n",
    "\n",
    "In addition, this is a good place to ask more general questions about AI development.\n",
    "\n",
    "## If You Get Stuck\n",
    "Just get someone's attention for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0BtdwFBr1pD"
   },
   "source": [
    "## Task 1: Run A Shell Command\n",
    "Lines of code that start with a `%` in a Jupyter Notebook execute a [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html). In the next cell, the magic command executes a bash command to show you what system you are running on.\n",
    "\n",
    "Run the code cell below by selecting it and using one of the following methods:\n",
    "* `shift + enter`: Run and move to the next cell\n",
    "* `ctrl + enter`: Run the cell without moving to the next cell\n",
    "* Press the run button in the toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1r4tOEt7r1pD",
    "outputId": "001901d0-62ff-410e-985a-03259d88c49a"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /etc/os-release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX3RRXzUr1pE"
   },
   "source": [
    "As seen above, Google Colab executes your code on Ubuntu Linux, currently the global standard for industrial-grade computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir7Mt3jgr1pF"
   },
   "source": [
    "## Task 2: Install Libraries\n",
    "### Context\n",
    "When you use a Jupyter Notebook for the first time, you will need to install the third-party libraries required to develop your software. Google Colab discards your computing environment after about 90 minutes of inactivity. So, in Colab, you must rerun your entire notebook, including library installation, whenever you reopen it.\n",
    "\n",
    "We are installing the following libraries:\n",
    "- `huggingface_hub`: The API we will use to access a hosted version of Falcon LLM\n",
    "- `dotenv`: Allows you to store your Hugging Face token securely\n",
    "- `langchain`: A popular library for making LLMs easier to use\n",
    "\n",
    "### How To Install The Libraries\n",
    "1. Option 1: Click the cell and press `shift + enter` to execute and move to the next cell\n",
    "2. Option 2: Click the run button in the jupyterhub toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h15kOFkHr1pF",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain huggingface_hub python-dotenv chainlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmirGE0Cr1pG"
   },
   "source": [
    "## Task 3: Get Hugging Face Access Token\n",
    "### Context\n",
    "To use an API, you typically need to generate a \"password\" for your code to log in to your account. This \"password\" is called an \"Access Token\".\n",
    "\n",
    "The canonical way to manage this password is to store it in a file called `.env`. \n",
    "\n",
    "Why not simply paste the password in your code? Primarily because it's easy to forget that password is there and then accidentally share the code on Github or elsewhere. Then, a hacker will quickly compromise your account using an automated scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmirGE0Cr1pG"
   },
   "source": [
    "### Instructions: Hugging Face Access Token\n",
    "First, run the `bash` command below to create a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "touch .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmirGE0Cr1pG"
   },
   "source": [
    "Next, create an access token on your Hugging Face account:\n",
    "1. Create an account at https://huggingface.co\n",
    "2. Go to https://huggingface.co/settings/token\n",
    "3. Click on the \"New Token\" button\n",
    "4. For **Name** put intro-to-full-stack-llm-token. For **Role**, put Read.\n",
    "5. Click \"Generate Token\"\n",
    "6. Copy the token to your clipboard\n",
    "7. Paste the token below, replacing `your_hugging_face_token`\n",
    "8. Run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwK0GevEr1pG"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"HUGGINGFACE_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" > .env\n",
    "\n",
    "cat .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were successful, you should see something like `HUGGINGFACE_API_TOKEN=hf_xxxxxxxxxxxxx` in the output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdO_u6YWr1pH"
   },
   "source": [
    "## Task 4: Load Hugging Face Token Into Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the access token, we need to load it from our `.env` file on disk into memory. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZNy_w9Rr1pH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token=os.getenv('HUGGINGFACE_API_TOKEN')\n",
    "print(\"Your access token: \" + token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the cell above should look like `Your access token: hf_xxxxxxxxxxxxx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJzFfd6Pr1pH",
    "tags": []
   },
   "source": [
    "## Task 5: Setup Hosted LLM\n",
    "### Context\n",
    "The model we will use today is [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct).\n",
    "\n",
    "**Falcon** is what TII of the UAE government named this model, fittingly because they like falcons. The Falcon model was the highest performing LLM on the [Hugging Face LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). It has since been surpassed by Facebook's Llama 2 model and others.\n",
    "\n",
    "**7b** refers to the number of parameters that the model has. A more resource-intensive but powerful model called `falcon-40b` has 40b parameters. So, the model we will use today is not nearly as good as ChatGPT. Why use it, then? Because this smaller model can be hosted on Hugging Face's servers for free. We'd need to pay for server time if we used a more powerful model like `falcon-40b` or `llama 2`. It's not very expensive, but it would have added more steps to this tutorial.\n",
    "\n",
    "**instruct** refers to the fact that the LLM is \"[instruction fine-tuned](https://arxiv.org/pdf/2109.01652.pdf),\" which means that the model has been specially fine-tuned to have the UX of a human assistant. Non-instruction fine-tuned LLMs are more challenging to use.\n",
    "\n",
    "### Instructions\n",
    "Run the code cell below.\n",
    "\n",
    "Read the comments explaining what each line does. Ask someone if something doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRwOH7Qkr1pH"
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub  # Allows us to use LLMs from Hugging Face Hub\n",
    "\n",
    "# We set up an LLM for use in the next task\n",
    "llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=token,      # Your \"password\"\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\", # The LLM we chose\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,   # Adjusts how \"creative\" the model will be\n",
    "        \"max_new_tokens\":200 # Maximum number of tokens the model will output\n",
    "    }\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hsu89tKAr1pI"
   },
   "source": [
    "## Task 6: Prompt Falcon-7B\n",
    "### Context\n",
    "Now, you can prompt Falcon-7B from the Python interpreter, similar to how you prompt ChatGPT from its GUI interface.\n",
    "\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Understand the comments explaining the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4y9cCtLr1pI"
   },
   "outputs": [],
   "source": [
    "# PromptTemplate is a feature of LangChain for defining reusable prompts\n",
    "#\n",
    "# LLMChain is called \"chain\" because LangChain started as a\n",
    "# library for \"chaining together\" multiple successive LLM calls\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables= [\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"\"\"\n",
    "Give me step by step instructions to cook pasta\n",
    "\"\"\"\n",
    "\n",
    "response = llm_chain.run(question) # Ask falcon-7b our question\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the output above looks reasonable. One quirk we have noticed is that `falcon-7b-instruct` tends to leave an extra token, \"User\" at the end of its responses. The extraneous token may be a shortcoming of its instruction fine-tuning process, which may have had many prompts with the format:  \n",
    "```\n",
    "User: {question the user asks}\n",
    "Assistant: {what falcon-7b-instruct response with}\n",
    "User:\n",
    "```\n",
    "\n",
    "This quirk hints at how simplistic LLMs truly are. They simply keep parroting what they expect should come next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1PNYmyWr1pI"
   },
   "source": [
    "## Task 7: Make Another Prompt\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Modify the code to make any other queries\n",
    "3. Compare the performance against ChatGPT in a separate window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKknAX8mr1pI"
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "What is the capital of British Columbia?\n",
    "\"\"\"\n",
    "\n",
    "completion = llm_chain.run(question)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NK_oVu3r1pJ"
   },
   "source": [
    "## Task 9: Add Hugging Face Access Token To Your Personal Computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next task, you will run the Python code from your personal computer. So, you will need to create a `.env` file with your access token there too.\n",
    "\n",
    "#### Instructions\n",
    "Do the following on your personal computer. Use the instructions for your operating system.\n",
    "\n",
    "#### Windows\n",
    "1. Open a terminal by searching for \"cmd\" in the Start Menu\n",
    "2. Change directory to the `workshops` directory\n",
    "```\n",
    "cd %USERPROFILE%/src/workshops\n",
    "```\n",
    "3. Create a `.env` file\n",
    "```\n",
    "echo.> .env\n",
    "```\n",
    "4. Open `.env` in Notepad\n",
    "```\n",
    "notepad .env\n",
    "```\n",
    "5. Paste your Hugging Face access token from earlier in the notebook in the file. Save the file and exit.\n",
    "\n",
    "#### Mac and Ubuntu Linux\n",
    "1. Open a terminal\n",
    "2. Change directory to the `workshops` directory\n",
    "```\n",
    "cd ~/src/workshops\n",
    "```\n",
    "3. Create a `.env` file\n",
    "```\n",
    "touch .env\n",
    "```\n",
    "4. Save your Hugging Face access token from earlier in the notebook in the file. Replace the x's with your real token.\n",
    "```\n",
    "echo \"HUGGINGFACE_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" > .env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Open The Chainlit UI Server File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a GUI to `falcon-7b-instruct` so that we can use it like ChatGPT's website.\n",
    "\n",
    "We will use a web UI library called [Chainlit](https://docs.chainlit.io/overview) to accomplish this. Chainlit describes itself as follows:\n",
    "> Chainlit is an open-source Python package that makes it incredibly fast to build and share LLM apps. Integrate the Chainlit API in your existing code to spawn a ChatGPT-like interface in minutes!\n",
    "\n",
    "That sounds like exactly what we want!\n",
    "\n",
    "We can't run the Chainlit web server from Colab, so we will walk through the whole file first, and then we will run it on your personal computer.\n",
    "\n",
    "### Instructions\n",
    "1. Navigate to your `~/src/workshops` folder using your file browser.\n",
    "2. Open `chainlit-falcon-7b-server.py` in your text editor.\n",
    "\n",
    "### Code Walkthrough\n",
    "This is the shebang. It's used to tell your operating system what script interpreter to use to run this file.\n",
    "```\n",
    "#!/usr/bin/env python3\n",
    "```\n",
    "\n",
    "Next are the imports. These usually go at the top of your program files in Python or any other language. The standard Python library imports are for libraries included with python itself, aka \"first party\". The other imports are libraries provided by third-party organizations like Hugging Face.\n",
    "```\n",
    "# Import standard Python library\n",
    "import os\n",
    "\n",
    "# Import third party libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain import HuggingFaceHub, LLMChain, PromptTemplate\n",
    "import chainlit as cl\n",
    "```\n",
    "\n",
    "This loads your Hugging Face access token. This is the exact same code you used before in this Jupyter Notebook.\n",
    "```\n",
    "# Load the environment variable for the Hugging Face token\n",
    "load_dotenv()\n",
    "token = os.getenv('HUGGINGFACE_API_TOKEN')\n",
    "```\n",
    "\n",
    "This sets up your LLM to use on Hugging Face's servers. This is the exact same code you used before in this Jupyter Notebook.\n",
    "```\n",
    "# Set up the Hugging Face LLM with Falcon-7B-Instruct\n",
    "llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=token,\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 200\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the prompt template for the Chainlit UI\n",
    "template = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "This is the first piece of new code from Chainlit UI. `on_chat_start` means that this is the code that runs when the chat window is first loaded by the user. If the code looks cryptic, it's because it is. Usually you'll get some basic code like this from the third-party developer and you'll modify it, so it's not important to memorize how to write code like this.\n",
    "```\n",
    "# Define the chat start event for Chainlit UI\n",
    "@cl.on_chat_start\n",
    "def on_chat_start():\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "```\n",
    "\n",
    "This code defines what the Chainlit UI does when a user sends a message using its UI. It does the following:\n",
    "1. Awaits the user sending it a message like \"What is the capital of British Columbia?\"\n",
    "2. Sends that message to the Hugging Face server hosting `falcon-7b-instruct`\n",
    "3. Awaits the Hugging Face server's response\n",
    "4. Prints that response for the user\n",
    "```\n",
    "# Define the message received event for Chainlit UI\n",
    "@cl.on_message\n",
    "async def on_message(message: str):\n",
    "    llm_chain = cl.user_session.get(\"llm_chain\")  # Retrieve the chain from the session\n",
    "    res = await llm_chain.acall(message, callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "    \n",
    "    # The response is sent back to the user\n",
    "    await cl.Message(content=res[\"text\"]).send()\n",
    "```\n",
    "\n",
    "Finally, this is a standard snippet of code included in many Python scripts. It simply tells the script what to do if you call it from the command line. You don't need to worry about this snippet for this workshop, but if you're interested, here are [the docs from Python Foundation](https://docs.python.org/3/library/__main__.html#idiomatic-usage).\n",
    "```\n",
    "if __name__ == \"__main__\":\n",
    "    # Chainlit does not require an explicit call to start the server, but you can add other initialization logic here if needed\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcmeZvmRr1pJ"
   },
   "source": [
    "## Task 11: Run The Web Server\n",
    "All that's left is to run the web server and use the code!\n",
    "\n",
    "### Instructions\n",
    "#### Windows\n",
    "1. Open a terminal by searching for \"cmd\" in the Start Menu\n",
    "2. Change directory to the `workshops` directory\n",
    "```\n",
    "cd %USERPROFILE%/src/workshops\n",
    "```\n",
    "3. Install third-party libraries using `requirements.txt`\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "4. Run `chainlit-falcon-7b-server.py`\n",
    "```\n",
    "chainlit run ./chainlit-falcon-7b-server.py\n",
    "```\n",
    "\n",
    "#### Mac\n",
    "1. Open a terminal\n",
    "2. Change directory to the `workshops` directory\n",
    "```\n",
    "cd ~/src/workshops\n",
    "```\n",
    "3. Install third-party libraries using `requirements.txt`\n",
    "```\n",
    "pip3 install -r requirements.txt\n",
    "```\n",
    "4. Run `chainlit-falcon-7b-server.py`\n",
    "```\n",
    "chainlit run ./chainlit-falcon-7b-server.py\n",
    "```\n",
    "\n",
    "#### Ubuntu\n",
    "1. Open a terminal\n",
    "2. Change directory to the `workshops` directory\n",
    "```\n",
    "cd ~/src/workshops\n",
    "```\n",
    "3. Create and activate a `venv`\n",
    "```\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "```\n",
    "3. Install third-party libraries using `requirements.txt`\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "4. Run `chainlit-falcon-7b-server.py`\n",
    "```\n",
    "chainlit run ./chainlit-falcon-7b-server.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
