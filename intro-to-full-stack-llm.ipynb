{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro To Full Stack LLM Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Goals\n",
    "Our goal is for you to understand:\n",
    "* How and why Iron Python notebooks are used for prototyping\n",
    "* Each section of code\n",
    "\n",
    "Overall, we hope you will feel enabled to do more advanced tutorials with standard data science tools in the future.\n",
    "\n",
    "## If You Get Stuck\n",
    "Just get someone's attention for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Run A Shell Command\n",
    "Lines of code that start with a `%` in an Iron Python notebook execute a `magic command`. In the next cell, the magic command executes a bash command to show you what system you are running on.\n",
    "\n",
    "Run the code cell below by selecting it and using one of the following methods:\n",
    "* `shift + enter`: run and move to next\n",
    "* `ctrl + enter`: run\n",
    "* Press the run button in the toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Install Libraries\n",
    "### Context\n",
    "Typically when you open a Juypter notebook file for the first time on JupyterHub, you will need to install the third-party libraries you will use to develop your software.\n",
    "\n",
    "We are installing the following libraries:\n",
    "- `huggingface_hub`: the API we will use to access a hosted version of Falcon LLM\n",
    "- `dotenv`: allows you to securely store your Hugging Face token\n",
    "- `langchain`: a popular library for making LLMs easier to use\n",
    "\n",
    "### How To\n",
    "1. Option 1: Click the cell and press `shift + enter` to execute and move to next cell\n",
    "2. Option 2: Click the run button in the jupyterhub toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain huggingface_hub python-dotenv chainlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Get Hugging Face Access Token\n",
    "### Context\n",
    "To use an API, you typically need to generate a \"password\" for your code to use to login to your account. This is typically called an \"Access Token\".\n",
    "\n",
    "To store this password securely, it's traditional to use a `.env` file so that the password doesn't accidentally get committed to a Git reposititory. So, we will run a `bash` command below to create this `.env` file.\n",
    "\n",
    "### Instructions\n",
    "1. Create an account at https://huggingface.co\n",
    "2. Go to https://huggingface.co/settings/token\n",
    "3. Click on \"New Token\" button\n",
    "4. For **Name** put intro-to-full-stack-llm-token. For **Role** put Read.\n",
    "5. Click generate token\n",
    "6. Copy the token to your clipboard\n",
    "7. Paste the token below, replacing `your_hugging_face_token`\n",
    "8. Run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"HUGGINGFACE_API_TOKEN={your_key}\" > .env\n",
    "\n",
    "cat .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Load Hugging Face Token Into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token=os.getenv('HUGGINGFACE_API_TOKEN')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 5: Setup Hosted LLM\n",
    "### Context\n",
    "The model we will use today is [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct).\n",
    "\n",
    "**Falcon** is what TII of the UAE government named this model, fittingly because they like falcons. For a while, the Falcon model was the highest performing LLM on the [Hugging Face LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). It has since been surpassed by Facebook's Llama 2 model and others.\n",
    "\n",
    "**7b** refers to the number of parameters that the model has. There is a more resource-intensive but powerful model called `falcon-40b` that has 40b parameters.\n",
    "\n",
    "**instruct** refers to the fact that the LLM is \"instruction fine-tuned,\" which means that the model has been specially fine-tuned to have the UX of a human assistant. Non-instruction fine-tuned LLMs are more difficult to use.\n",
    "\n",
    "### Instructions\n",
    "Run the code below.\n",
    "\n",
    "Read the comments explaining what each line does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub  # Allows us to use LLMs from Hugging Face Hub\n",
    "\n",
    "# We set up an LLM for use in the next task\n",
    "llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=token, # Your \"password\"\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7, # Adjusts how \"creative\" the model will be\n",
    "        \"max_new_tokens\":200 # Maximum number of tokens the model will output\n",
    "    }\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Prompt Falcon-7B\n",
    "### Context\n",
    "Now you can prompt Falcon-7B from the Python interpreter similar to how you prompt ChatGPT from its GUI interface.\n",
    "\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Understand the comments explaining the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate is a feature of LangChain for defining reusable prompts\n",
    "#\n",
    "# LLMChain is called \"chain\" because LangChain started as a \n",
    "# library for \"chaining together\" multiple successive LLM calls\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables= [\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"\"\"\n",
    "User: Give me step by step instructions to cook pasta\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Make Another Prompt\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Modify the code to make any other queries\n",
    "3. Compare the performance against ChatGPT in a separate window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "User: What is the capitol of British Columbia?\n",
    "\"\"\"\n",
    "\n",
    "completion = llm_chain.run(question)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Convert the notebook to a python file\n",
    "### Instructions\n",
    "1. Run the code below in your terminal\n",
    "2. Open the generated .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert notebook to py (add ! to run in cell)\n",
    "\n",
    "# jupyter nbconvert --to script intro-to-full-stack-llm.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Add Chainlit UI code to py file\n",
    "### Instructions\n",
    "1. Run the code below in your terminal\n",
    "2. Open the generated .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Adding Chainlit code to .py file\n",
    "\n",
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "import chainlit as cl\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "def main():\n",
    "    # Instantiate the chain for that user session\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\n",
    "\n",
    "    # Store the chain in the user session\n",
    "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: str):\n",
    "    # Retrieve the chain from the user session\n",
    "    llm_chain = cl.user_session.get(\"llm_chain\")  # type: LLMChain\n",
    "\n",
    "    # Call the chain asynchronously\n",
    "    res = await llm_chain.acall(message, callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "\n",
    "    # Do any post processing here\n",
    "\n",
    "    # \"res\" is a Dict. For this chain, we get the response by reading the \"text\" key.\n",
    "    # This varies from chain to chain, you should check which key to read.\n",
    "    await cl.Message(content=res[\"text\"]).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Run minified-chainlit.py file\n",
    "### Instructions\n",
    "1. Run the code below in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainlit run minified-chainlit.py -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
