{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1UpdIFjr1o_"
   },
   "source": [
    "# Intro To Full Stack LLM Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "i2lzOJhQr1pC",
    "tags": []
   },
   "source": [
    "## Workshop Goals\n",
    "Our goal is to give you an understanding of:\n",
    "- The basic components that go into building your own ChatGPT-like app\n",
    "- The basic workflow of prototyping an AI app in JupyterLab\n",
    "- How to convert a JupyterLab prototype into an app that can run on a web server\n",
    "- How to run your web server app on your computer\n",
    "\n",
    "In addition, this is a good place to ask more general questions about AI development.\n",
    "\n",
    "## If You Get Stuck\n",
    "Just get someone's attention for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0BtdwFBr1pD"
   },
   "source": [
    "## Task 1: Run A Shell Command\n",
    "Lines of code that start with a `%` in a Jupyter Notebook execute a [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html). In the next cell, the magic command executes a bash command to show you what system you are running on.\n",
    "\n",
    "Run the code cell below by selecting it and using one of the following methods:\n",
    "* `shift + enter`: Run and move to the next cell\n",
    "* `ctrl + enter`: Run the cell without moving to the next cell\n",
    "* Press the run button in the toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1r4tOEt7r1pD",
    "outputId": "001901d0-62ff-410e-985a-03259d88c49a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETTY_NAME=\"Ubuntu 23.04\"\n",
      "NAME=\"Ubuntu\"\n",
      "VERSION_ID=\"23.04\"\n",
      "VERSION=\"23.04 (Lunar Lobster)\"\n",
      "VERSION_CODENAME=lunar\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "UBUNTU_CODENAME=lunar\n",
      "LOGO=ubuntu-logo\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /etc/os-release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX3RRXzUr1pE"
   },
   "source": [
    "As seen above, Google Colab executes your code on Ubuntu Linux, currently the global standard for industrial-grade computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir7Mt3jgr1pF"
   },
   "source": [
    "## Task 2: Install Libraries\n",
    "### Context\n",
    "When you use a Jupyter Notebook for the first time, you will need to install the third-party libraries required to develop your software. Google Colab discards your computing environment after about 90 minutes of inactivity. So, in Colab, you must rerun your entire notebook, including library installation, whenever you reopen it.\n",
    "\n",
    "We are installing the following libraries:\n",
    "- `huggingface_hub`: The API we will use to access a hosted version of Falcon LLM\n",
    "- `dotenv`: Allows you to store your Hugging Face token securely\n",
    "- `langchain`: A popular library for making LLMs easier to use\n",
    "\n",
    "### How To Install The Libraries\n",
    "1. Option 1: Click the cell and press `shift + enter` to execute and move to the next cell\n",
    "2. Option 2: Click the run button in the jupyterhub toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h15kOFkHr1pF",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (0.0.279)\n",
      "Requirement already satisfied: huggingface_hub in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (0.16.4)\n",
      "Requirement already satisfied: python-dotenv in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: chainlit in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (0.6.402)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (0.0.33)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (2.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: filelock in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from huggingface_hub) (3.12.3)\n",
      "Requirement already satisfied: fsspec in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (23.2.1)\n",
      "Requirement already satisfied: asyncer<0.0.3,>=0.0.2 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (0.0.2)\n",
      "Requirement already satisfied: auth0-python<5.0.0,>=4.4.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (4.4.1)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (8.1.7)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.99.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (0.99.1)\n",
      "Requirement already satisfied: fastapi-socketio<0.0.11,>=0.0.10 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (0.0.10)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (1.2.0)\n",
      "Requirement already satisfied: lazify<0.5.0,>=0.4.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (0.4.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.6 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (1.5.7)\n",
      "Requirement already satisfied: prisma<0.11.0,>=0.10.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (0.10.0)\n",
      "Requirement already satisfied: python-graphql-client<0.5.0,>=0.4.3 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (0.4.3)\n",
      "Requirement already satisfied: syncer<3.0.0,>=2.0.3 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (2.0.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (2.0.1)\n",
      "Requirement already satisfied: uptrace<2.0.0,>=1.18.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (1.19.0)\n",
      "Requirement already satisfied: uvicorn<0.24.0,>=0.23.2 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (0.23.2)\n",
      "Requirement already satisfied: watchfiles<0.21.0,>=0.20.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from chainlit) (0.20.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.4.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from asyncer<0.0.3,>=0.0.2->chainlit) (3.7.1)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=41.0.3 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from auth0-python<5.0.0,>=4.4.0->chainlit) (41.0.3)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from auth0-python<5.0.0,>=4.4.0->chainlit) (2.8.0)\n",
      "Requirement already satisfied: pyopenssl<24.0.0,>=23.2.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from auth0-python<5.0.0,>=4.4.0->chainlit) (23.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from fastapi<0.100.0,>=0.99.0->chainlit) (0.27.0)\n",
      "Requirement already satisfied: python-socketio>=4.6.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from fastapi-socketio<0.0.11,>=0.0.10->chainlit) (5.8.0)\n",
      "Requirement already satisfied: httpx>=0.19.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from prisma<0.11.0,>=0.10.0->chainlit) (0.24.1)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from prisma<0.11.0,>=0.10.0->chainlit) (3.1.2)\n",
      "Requirement already satisfied: tomlkit in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from prisma<0.11.0,>=0.10.0->chainlit) (0.12.1)\n",
      "Requirement already satisfied: nodeenv in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from prisma<0.11.0,>=0.10.0->chainlit) (1.8.0)\n",
      "Requirement already satisfied: websockets>=5.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from python-graphql-client<0.5.0,>=0.4.3->chainlit) (11.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: opentelemetry-api==1.19.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp==1.19.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.40b0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.18.0->chainlit) (0.40b0)\n",
      "Requirement already satisfied: opentelemetry-sdk==1.19.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-api==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata~=6.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-api==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (6.8.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.19.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.19.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.40b0->uptrace<2.0.0,>=1.18.0->chainlit) (66.1.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.40b0->uptrace<2.0.0,>=1.18.0->chainlit) (1.15.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.40b0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-sdk==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (0.40b0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.60.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.57.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.19.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.19.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.19 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from opentelemetry-proto==1.19.0->opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (4.24.2)\n",
      "Requirement already satisfied: h11>=0.8 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from uvicorn<0.24.0,>=0.23.2->chainlit) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from anyio<4.0.0,>=3.4.0->asyncer<0.0.3,>=0.0.2->chainlit) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from cryptography<42.0.0,>=41.0.3->auth0-python<5.0.0,>=4.4.0->chainlit) (1.15.1)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from httpx>=0.19.0->prisma<0.11.0,>=0.10.0->chainlit) (0.17.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from jinja2>=2.11.2->prisma<0.11.0,>=0.10.0->chainlit) (2.1.3)\n",
      "Requirement already satisfied: bidict>=0.21.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit) (0.22.1)\n",
      "Requirement already satisfied: python-engineio>=4.3.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit) (4.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.3->auth0-python<5.0.0,>=4.4.0->chainlit) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages (from importlib-metadata~=6.0->opentelemetry-api==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (3.16.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain huggingface_hub python-dotenv chainlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmirGE0Cr1pG"
   },
   "source": [
    "## Task 3: Get Hugging Face Access Token\n",
    "### Context\n",
    "To use an API, you typically need to generate a \"password\" for your code to log in to your account. This \"password\" is called an \"Access Token\".\n",
    "\n",
    "The canonical way to manage this password is to store it in a file called `.env`. \n",
    "\n",
    "Why not simply paste the password in your code? Primarily because it's easy to forget that password is there and then accidentally share the code on Github or elsewhere. Then, a hacker will quickly compromise your account using an automated scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmirGE0Cr1pG"
   },
   "source": [
    "### Instructions: Hugging Face Access Token\n",
    "First, run the `bash` command below to create a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "touch .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmirGE0Cr1pG"
   },
   "source": [
    "Next, create an access token on your Hugging Face account:\n",
    "1. Create an account at https://huggingface.co\n",
    "2. Go to https://huggingface.co/settings/token\n",
    "3. Click on the \"New Token\" button\n",
    "4. For **Name** put intro-to-full-stack-llm-token. For **Role**, put Read.\n",
    "5. Click \"Generate Token\"\n",
    "6. Copy the token to your clipboard\n",
    "7. Paste the token below, replacing `your_hugging_face_token`\n",
    "8. Run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NwK0GevEr1pG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUGGINGFACE_API_TOKEN=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\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"HUGGINGFACE_API_TOKEN=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\" > .env\n",
    "\n",
    "cat .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were successful, you should see something like `HUGGINGFACE_API_TOKEN=hf_xxxxxxxxxxxxx` in the output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdO_u6YWr1pH"
   },
   "source": [
    "## Task 4: Load Hugging Face Token Into Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the access token, we need to load it from our `.env` file on disk into memory. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HZNy_w9Rr1pH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your access token: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token=os.getenv('HUGGINGFACE_API_TOKEN')\n",
    "print(\"Your access token: \" + token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the cell above should look like `Your access token: hf_xxxxxxxxxxxxx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJzFfd6Pr1pH",
    "tags": []
   },
   "source": [
    "## Task 5: Setup Hosted LLM\n",
    "### Context\n",
    "The model we will use today is [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct).\n",
    "\n",
    "**Falcon** is what TII of the UAE government named this model, fittingly because they like falcons. The Falcon model was the highest performing LLM on the [Hugging Face LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). It has since been surpassed by Facebook's Llama 2 model and others.\n",
    "\n",
    "**7b** refers to the number of parameters that the model has. A more resource-intensive but powerful model called `falcon-40b` has 40b parameters. So, the model we will use today is not nearly as good as ChatGPT. Why use it, then? Because this smaller model can be hosted on Hugging Face's servers for free. We'd need to pay for server time if we used a more powerful model like `falcon-40b` or `llama 2`. It's not very expensive, but it would have added more steps to this tutorial.\n",
    "\n",
    "**instruct** refers to the fact that the LLM is \"[instruction fine-tuned](https://arxiv.org/pdf/2109.01652.pdf),\" which means that the model has been specially fine-tuned to have the UX of a human assistant. Non-instruction fine-tuned LLMs are more challenging to use.\n",
    "\n",
    "### Instructions\n",
    "Run the code cell below.\n",
    "\n",
    "Read the comments explaining what each line does. Ask someone if something doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CRwOH7Qkr1pH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'tiiuae/falcon-7b-instruct', 'task': None, 'model_kwargs': {'temperature': 0.7, 'max_new_tokens': 200}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toby/src/tobkin/jupyterlab-ubuntu-local/jupyterlab-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub  # Allows us to use LLMs from Hugging Face Hub\n",
    "\n",
    "# We set up an LLM for use in the next task\n",
    "llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=token,      # Your \"password\"\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\", # The LLM we chose\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,   # Adjusts how \"creative\" the model will be\n",
    "        \"max_new_tokens\":200 # Maximum number of tokens the model will output\n",
    "    }\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hsu89tKAr1pI"
   },
   "source": [
    "## Task 6: Prompt Falcon-7B\n",
    "### Context\n",
    "Now, you can prompt Falcon-7B from the Python interpreter, similar to how you prompt ChatGPT from its GUI interface.\n",
    "\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Understand the comments explaining the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x4y9cCtLr1pI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Fill a large pot with water and bring it to a boil.\n",
      "2. Add salt to the water (about 2-3 tablespoons per 4 cups of water). \n",
      "3. Add the pasta to the boiling water and cook for 8-10 minutes. \n",
      "4. Use a fork to stir the pasta to prevent sticking.\n",
      "5. Drain the pasta using a colander and return it to the pot.\n",
      "6. If desired, add sauce and any desired toppings.\n",
      "7. Enjoy your delicious pasta!\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate is a feature of LangChain for defining reusable prompts\n",
    "#\n",
    "# LLMChain is called \"chain\" because LangChain started as a\n",
    "# library for \"chaining together\" multiple successive LLM calls\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables= [\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"\"\"\n",
    "Give me step by step instructions to cook pasta\n",
    "\"\"\"\n",
    "\n",
    "response = llm_chain.run(question) # Ask falcon-7b our question\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the output above looks reasonable. One quirk we have noticed is that `falcon-7b-instruct` tends to leave an extra token, \"User\" at the end of its responses. The extraneous token may be a shortcoming of its instruction fine-tuning process, which may have had many prompts with the format:  \n",
    "```\n",
    "User: {question the user asks}\n",
    "Assistant: {what falcon-7b-instruct response with}\n",
    "User:\n",
    "```\n",
    "\n",
    "This quirk hints at how simplistic LLMs truly are. They simply keep parroting what they expect should come next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1PNYmyWr1pI"
   },
   "source": [
    "## Task 7: Make Another Prompt\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Modify the code to make any other queries\n",
    "3. Compare the performance against ChatGPT in a separate window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aKknAX8mr1pI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>The capital of British Columbia is Victoria. </p>\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "What is the capital of British Columbia?\n",
    "\"\"\"\n",
    "\n",
    "completion = llm_chain.run(question)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NK_oVu3r1pJ"
   },
   "source": [
    "## Task 9: Add Hugging Face Access Token To Your Personal Computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next task, you will run the Python code from your personal computer. So, you will need to create a `.env` file with your access token there too.\n",
    "\n",
    "#### Instructions\n",
    "Do the following on your personal computer. Use the instructions for your operating system.\n",
    "\n",
    "#### Windows\n",
    "1. Open a terminal by searching for \"cmd\" in the Start Menu\n",
    "2. Change directory to the `workshops` directory\n",
    "```\n",
    "cd %USERPROFILE%/src/workshops\n",
    "```\n",
    "3. Create a `.env` file\n",
    "```\n",
    "echo.> .env\n",
    "```\n",
    "4. Open `.env` in Notepad\n",
    "```\n",
    "notepad .env\n",
    "```\n",
    "5. Paste your Hugging Face access token from earlier in the notebook in the file. Save the file and exit.\n",
    "\n",
    "#### Mac and Ubuntu Linux\n",
    "1. Open a terminal\n",
    "2. Change directory to the `workshops` directory\n",
    "```\n",
    "cd ~/src/workshops\n",
    "```\n",
    "3. Create a `.env` file\n",
    "```\n",
    "touch .env\n",
    "```\n",
    "4. Save your Hugging Face access token from earlier in the notebook in the file. Replace the x's with your real token.\n",
    "```\n",
    "echo \"HUGGINGFACE_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" > .env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Open "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will add a GUI to `falcon-7b-instruct` so that we can use it like ChatGPT's website.\n",
    "\n",
    "We will use a web UI library called [Chainlit](https://docs.chainlit.io/overview) to accomplish this. Chainlit describes itself as follows:\n",
    "> Chainlit is an open-source Python package that makes it incredibly fast to build and share LLM apps. Integrate the Chainlit API in your existing code to spawn a ChatGPT-like interface in minutes!\n",
    "\n",
    "That sounds like exactly what we want!\n",
    "\n",
    "We can't run the Chainlit web server from Colab, so we will walk through the whole file first, and then we will run it on your personal computer.\n",
    "\n",
    "### Instructions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPGx_Y4er1pJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Adding Chainlit code to .py file\n",
    "\n",
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "import chainlit as cl\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "def main():\n",
    "    # Instantiate the chain for that user session\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\n",
    "\n",
    "    # Store the chain in the user session\n",
    "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: str):\n",
    "    # Retrieve the chain from the user session\n",
    "    llm_chain = cl.user_session.get(\"llm_chain\")  # type: LLMChain\n",
    "\n",
    "    # Call the chain asynchronously\n",
    "    res = await llm_chain.acall(message, callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "\n",
    "    # Do any post processing here\n",
    "\n",
    "    # \"res\" is a Dict. For this chain, we get the response by reading the \"text\" key.\n",
    "    # This varies from chain to chain, you should check which key to read.\n",
    "    await cl.Message(content=res[\"text\"]).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcmeZvmRr1pJ"
   },
   "source": [
    "## Task 10: Run minified-chainlit.py file\n",
    "### Instructions\n",
    "1. Run the code below in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLwGKIBgr1pJ"
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfo0S1Wyr1pJ"
   },
   "outputs": [],
   "source": [
    "chainlit run minified-chainlit.py -w"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
