{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro To Full Stack LLM Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Goals\n",
    "Our goal is for you to understand:\n",
    "* How and why Iron Python notebooks are used for prototyping\n",
    "* Each section of code\n",
    "\n",
    "Overall, we hope you will feel enabled to do more advanced tutorials with standard data science tools in the future.\n",
    "\n",
    "## If You Get Stuck\n",
    "Just get someone's attention for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Run A Shell Command\n",
    "Lines of code that start with a `%` in an Iron Python notebook execute a `magic command`. In the next cell, the magic command executes a bash command to show you what system you are running on.\n",
    "\n",
    "Run the code cell below by selecting it and using one of the following methods:\n",
    "* `shift + enter`: run and move to next\n",
    "* `ctrl + enter`: run\n",
    "* Press the run button in the toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LSB modules are available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 22.04.2 LTS\n",
      "Release:\t22.04\n",
      "Codename:\tjammy\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "lsb_release -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LSB modules are available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 22.04.2 LTS\n",
      "Release:\t22.04\n",
      "Codename:\tjammy\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_cell_magic('bash', '', 'lsb_release -a\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Install Libraries\n",
    "### Context\n",
    "Typically when you open a Juypter notebook file for the first time on JupyterHub, you will need to install the third-party libraries you will use to develop your software.\n",
    "\n",
    "We are installing the following libraries:\n",
    "- `huggingface_hub`: the API we will use to access a hosted version of Falcon LLM\n",
    "- `dotenv`: allows you to securely store your Hugging Face token\n",
    "- `langchain`: a popular library for making LLMs easier to use\n",
    "\n",
    "### How To\n",
    "1. Option 1: Click the cell and press `shift + enter` to execute and move to next cell\n",
    "2. Option 2: Click the run button in the jupyterhub toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in ./.local/lib/python3.10/site-packages (0.0.277)\n",
      "Requirement already satisfied: huggingface_hub in ./.local/lib/python3.10/site-packages (0.16.4)\n",
      "Requirement already satisfied: python-dotenv in ./.local/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: chainlit in ./.local/lib/python3.10/site-packages (0.6.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.local/lib/python3.10/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./.local/lib/python3.10/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in ./.local/lib/python3.10/site-packages (from langchain) (0.0.29)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./.local/lib/python3.10/site-packages (from langchain) (2.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.local/lib/python3.10/site-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.local/lib/python3.10/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.local/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.local/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from huggingface_hub) (3.12.3)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/tljh/user/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.10/site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/tljh/user/lib/python3.10/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in ./.local/lib/python3.10/site-packages (from chainlit) (23.2.1)\n",
      "Requirement already satisfied: asyncer<0.0.3,>=0.0.2 in ./.local/lib/python3.10/site-packages (from chainlit) (0.0.2)\n",
      "Requirement already satisfied: auth0-python<5.0.0,>=4.4.0 in ./.local/lib/python3.10/site-packages (from chainlit) (4.4.1)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in ./.local/lib/python3.10/site-packages (from chainlit) (8.1.7)\n",
      "Requirement already satisfied: fastapi<0.98.0,>=0.97.0 in ./.local/lib/python3.10/site-packages (from chainlit) (0.97.0)\n",
      "Requirement already satisfied: fastapi-socketio<0.0.11,>=0.0.10 in ./.local/lib/python3.10/site-packages (from chainlit) (0.0.10)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./.local/lib/python3.10/site-packages (from chainlit) (1.2.0)\n",
      "Requirement already satisfied: lazify<0.5.0,>=0.4.0 in ./.local/lib/python3.10/site-packages (from chainlit) (0.4.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.6 in /opt/tljh/user/lib/python3.10/site-packages (from chainlit) (1.5.7)\n",
      "Requirement already satisfied: prisma<0.10.0,>=0.9.0 in ./.local/lib/python3.10/site-packages (from chainlit) (0.9.1)\n",
      "Requirement already satisfied: python-graphql-client<0.5.0,>=0.4.3 in ./.local/lib/python3.10/site-packages (from chainlit) (0.4.3)\n",
      "Requirement already satisfied: syncer<3.0.0,>=2.0.3 in ./.local/lib/python3.10/site-packages (from chainlit) (2.0.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /opt/tljh/user/lib/python3.10/site-packages (from chainlit) (2.0.1)\n",
      "Requirement already satisfied: uptrace<2.0.0,>=1.18.0 in ./.local/lib/python3.10/site-packages (from chainlit) (1.19.0)\n",
      "Requirement already satisfied: uvicorn<0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from chainlit) (0.22.0)\n",
      "Requirement already satisfied: watchfiles<0.20.0,>=0.19.0 in ./.local/lib/python3.10/site-packages (from chainlit) (0.19.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/tljh/user/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.4.0 in /opt/tljh/user/lib/python3.10/site-packages (from asyncer<0.0.3,>=0.0.2->chainlit) (3.7.1)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=41.0.3 in ./.local/lib/python3.10/site-packages (from auth0-python<5.0.0,>=4.4.0->chainlit) (41.0.3)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in ./.local/lib/python3.10/site-packages (from auth0-python<5.0.0,>=4.4.0->chainlit) (2.8.0)\n",
      "Requirement already satisfied: pyopenssl<24.0.0,>=23.2.0 in ./.local/lib/python3.10/site-packages (from auth0-python<5.0.0,>=4.4.0->chainlit) (23.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./.local/lib/python3.10/site-packages (from fastapi<0.98.0,>=0.97.0->chainlit) (0.27.0)\n",
      "Requirement already satisfied: python-socketio>=4.6.0 in ./.local/lib/python3.10/site-packages (from fastapi-socketio<0.0.11,>=0.0.10->chainlit) (5.8.0)\n",
      "Requirement already satisfied: httpx>=0.19.0 in ./.local/lib/python3.10/site-packages (from prisma<0.10.0,>=0.9.0->chainlit) (0.24.1)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /opt/tljh/user/lib/python3.10/site-packages (from prisma<0.10.0,>=0.9.0->chainlit) (3.1.2)\n",
      "Requirement already satisfied: tomlkit in ./.local/lib/python3.10/site-packages (from prisma<0.10.0,>=0.9.0->chainlit) (0.12.1)\n",
      "Requirement already satisfied: nodeenv in ./.local/lib/python3.10/site-packages (from prisma<0.10.0,>=0.9.0->chainlit) (1.8.0)\n",
      "Requirement already satisfied: websockets>=5.0 in ./.local/lib/python3.10/site-packages (from python-graphql-client<0.5.0,>=0.4.3->chainlit) (11.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/tljh/user/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: opentelemetry-api==1.19.0 in ./.local/lib/python3.10/site-packages (from uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp==1.19.0 in ./.local/lib/python3.10/site-packages (from uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.40b0 in ./.local/lib/python3.10/site-packages (from uptrace<2.0.0,>=1.18.0->chainlit) (0.40b0)\n",
      "Requirement already satisfied: opentelemetry-sdk==1.19.0 in ./.local/lib/python3.10/site-packages (from uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.local/lib/python3.10/site-packages (from opentelemetry-api==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata~=6.0 in ./.local/lib/python3.10/site-packages (from opentelemetry-api==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (6.8.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.19.0 in ./.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.19.0 in ./.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /opt/tljh/user/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.40b0->uptrace<2.0.0,>=1.18.0->chainlit) (65.6.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./.local/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.40b0->uptrace<2.0.0,>=1.18.0->chainlit) (1.15.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.40b0 in ./.local/lib/python3.10/site-packages (from opentelemetry-sdk==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (0.40b0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in ./.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.60.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in ./.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.57.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.19.0 in ./.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.19.0 in ./.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (1.19.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.19 in ./.local/lib/python3.10/site-packages (from opentelemetry-proto==1.19.0->opentelemetry-exporter-otlp-proto-grpc==1.19.0->opentelemetry-exporter-otlp==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (4.24.2)\n",
      "Requirement already satisfied: h11>=0.8 in ./.local/lib/python3.10/site-packages (from uvicorn<0.23.0,>=0.22.0->chainlit) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/tljh/user/lib/python3.10/site-packages (from anyio<4.0.0,>=3.4.0->asyncer<0.0.3,>=0.0.2->chainlit) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/tljh/user/lib/python3.10/site-packages (from anyio<4.0.0,>=3.4.0->asyncer<0.0.3,>=0.0.2->chainlit) (1.1.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/tljh/user/lib/python3.10/site-packages (from cryptography<42.0.0,>=41.0.3->auth0-python<5.0.0,>=4.4.0->chainlit) (1.15.1)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in ./.local/lib/python3.10/site-packages (from httpx>=0.19.0->prisma<0.10.0,>=0.9.0->chainlit) (0.17.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.10/site-packages (from jinja2>=2.11.2->prisma<0.10.0,>=0.9.0->chainlit) (2.1.3)\n",
      "Requirement already satisfied: bidict>=0.21.0 in ./.local/lib/python3.10/site-packages (from python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit) (0.22.1)\n",
      "Requirement already satisfied: python-engineio>=4.3.0 in ./.local/lib/python3.10/site-packages (from python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit) (4.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/tljh/user/lib/python3.10/site-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.3->auth0-python<5.0.0,>=4.4.0->chainlit) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.local/lib/python3.10/site-packages (from importlib-metadata~=6.0->opentelemetry-api==1.19.0->uptrace<2.0.0,>=1.18.0->chainlit) (3.16.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain huggingface_hub python-dotenv chainlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Get Hugging Face Access Token\n",
    "### Context\n",
    "To use an API, you typically need to generate a \"password\" for your code to use to login to your account. This is typically called an \"Access Token\".\n",
    "\n",
    "To store this password securely, it's traditional to use a `.env` file so that the password doesn't accidentally get committed to a Git reposititory. So, we will run a `bash` command below to create this `.env` file.\n",
    "\n",
    "### Instructions\n",
    "1. Create an account at https://huggingface.co\n",
    "2. Go to https://huggingface.co/settings/token\n",
    "3. Click on \"New Token\" button\n",
    "4. For **Name** put intro-to-full-stack-llm-token. For **Role** put Read.\n",
    "5. Click generate token\n",
    "6. Copy the token to your clipboard\n",
    "7. Paste the token below, replacing `your_hugging_face_token`\n",
    "8. Run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUGGINGFACE_API_TOKEN=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\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"HUGGINGFACE_API_TOKEN=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\" > .env\n",
    "\n",
    "cat .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Load Hugging Face Token Into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token=os.getenv('HUGGINGFACE_API_TOKEN')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 5: Setup Hosted LLM\n",
    "### Context\n",
    "The model we will use today is [falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct).\n",
    "\n",
    "**Falcon** is what TII of the UAE government named this model, fittingly because they like falcons. For a while, the Falcon model was the highest performing LLM on the [Hugging Face LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). It has since been surpassed by Facebook's Llama 2 model and others.\n",
    "\n",
    "**7b** refers to the number of parameters that the model has. There is a more resource-intensive but powerful model called `falcon-40b` that has 40b parameters.\n",
    "\n",
    "**instruct** refers to the fact that the LLM is \"instruction fine-tuned,\" which means that the model has been specially fine-tuned to have the UX of a human assistant. Non-instruction fine-tuned LLMs are much more difficult to use.\n",
    "\n",
    "### Instructions\n",
    "Run the code below.\n",
    "\n",
    "Read the comments explaining what each line does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'tiiuae/falcon-7b-instruct', 'task': None, 'model_kwargs': {'temperature': 0.7, 'max_new_tokens': 200}}\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub  # Allows us to use LLMs from Hugging Face Hub\n",
    "\n",
    "# We set up an LLM for use in the next task\n",
    "llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=token, # Your \"password\"\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7, # Adjusts how \"random\" or \"deterministic\" the model will output\n",
    "        \"max_new_tokens\":200 # Maximum number of tokens the model will output\n",
    "    }\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Prompt Falcon-7B\n",
    "### Context\n",
    "Now you can prompt Falcon-7B from the Python interpreter similar to how you prompt ChatGPT from its GUI interface.\n",
    "\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Understand the comments explaining the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Bring a large pot of salted water to a boil.\n",
      "- Add the desired amount of pasta to the boiling water and cook for 2-4 minutes, stirring occasionally.\n",
      "- Remove the pasta from the pot using a slotted spoon and place it in a colander to drain.\n",
      "- Add desired sauce and mix well.\n",
      "- Serve and enjoy!\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate is a feature of LangChain for defining reusable prompts\n",
    "#\n",
    "# LLMChain is called \"chain\" because LangChain started as a \n",
    "# library for \"chaining together\" multiple successive LLM calls\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables= [\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"\"\"\n",
    "User: Give me step by step instructions to cook pasta\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Make Another Prompt\n",
    "### Instructions\n",
    "1. Run the code below\n",
    "2. Modify the code to make any other queries\n",
    "3. Compare the performance against ChatGPT in a separate window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>The capitol of British Columbia is Victoria. </p>\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "User: What is the capitol of British Columbia?\n",
    "\"\"\"\n",
    "\n",
    "completion = llm_chain.run(question)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Convert the notebook to a python file\n",
    "### Instructions\n",
    "1. Run the code below in your terminal\n",
    "2. Open the generated .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1803013326.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    jupyter nbconvert --to script intro-to-full-stack-llm.ipynb\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Convert notebook to py (add ! to run in cell)\n",
    "\n",
    "# jupyter nbconvert --to script intro-to-full-stack-llm.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Add Chainlit UI code to py file\n",
    "### Instructions\n",
    "1. Run the code below in your terminal\n",
    "2. Open the generated .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:55:57 - Created default config file at /home/jupyter-user1/LLM_workshop_1_notebooks/.chainlit/config.toml\n",
      "2023-08-11 17:55:57 - Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "## Adding Chainlit code to .py file\n",
    "\n",
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "import chainlit as cl\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "def main():\n",
    "    # Instantiate the chain for that user session\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\n",
    "\n",
    "    # Store the chain in the user session\n",
    "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: str):\n",
    "    # Retrieve the chain from the user session\n",
    "    llm_chain = cl.user_session.get(\"llm_chain\")  # type: LLMChain\n",
    "\n",
    "    # Call the chain asynchronously\n",
    "    res = await llm_chain.acall(message, callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "\n",
    "    # Do any post processing here\n",
    "\n",
    "    # \"res\" is a Dict. For this chain, we get the response by reading the \"text\" key.\n",
    "    # This varies from chain to chain, you should check which key to read.\n",
    "    await cl.Message(content=res[\"text\"]).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Run minified-chainlit.py file\n",
    "### Instructions\n",
    "1. Run the code below in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainlit run minified-chainlit.py -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
